{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f71748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report  \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications import Xception\n",
    "from keras.applications import ResNet50V2\n",
    "from keras.applications import ResNet101\n",
    "from keras.applications import ResNet152\n",
    "from keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188b5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'rgb' #'grayscale' o 'rgb'\n",
    "\n",
    "escala = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376c8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(path1, path2, escala = escala, color = color):\n",
    "    \n",
    "    EMD = os.listdir(path1)\n",
    "    NO_EMD = os.listdir(path2)\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i in EMD:   \n",
    "        image=tf.keras.preprocessing.image.load_img(path1+'/'+i, color_mode= color, \n",
    "        target_size= (escala, escala))\n",
    "        image=np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(1)\n",
    "    for i in NO_EMD:   \n",
    "        image=tf.keras.preprocessing.image.load_img(path2+'/'+i, color_mode= color, \n",
    "        target_size= (escala, escala))\n",
    "        image=np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(0)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949775c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iphone = cargar_datos('Datos preprocesados INP EMD/iPhone/EMD', 'Datos preprocesados INP EMD/iPhone/NO EMD')\n",
    "\n",
    "dataset_samsung = cargar_datos('Datos preprocesados INP EMD/Samsung/EMD', 'Datos preprocesados INP EMD/Samsung/NO EMD')\n",
    "\n",
    "dataset = cargar_datos('Datos preprocesados INP EMD/OCT/EMD', 'Datos preprocesados INP EMD/OCT/NO EMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8db162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trasnferLearning_classweight(test, red):\n",
    "    \n",
    "    #Definir conjuntos de datos train y test\n",
    "    \n",
    "    if test=='iphone':\n",
    "        \n",
    "        train_ds = np.concatenate((dataset[0],dataset_samsung[0]))\n",
    "        train_labels = np.concatenate((dataset[1],dataset_samsung[1]))\n",
    "        \n",
    "        test_ds = dataset_iphone[0]\n",
    "        test_labels = dataset_iphone[1]\n",
    "        \n",
    "    elif test=='samsung':\n",
    "        \n",
    "        train_ds = np.concatenate((dataset[0],dataset_iphone[0]))\n",
    "        train_labels = np.concatenate((dataset[1],dataset_iphone[1]))\n",
    "\n",
    "        test_ds = dataset_samsung[0]\n",
    "        test_labels = dataset_samsung[1]\n",
    "    \n",
    "    train_labels_categorical = to_categorical(train_labels, num_classes=2)\n",
    "    test_labels_categorical = to_categorical(test_labels, num_classes=2)\n",
    "    \n",
    "    #Definir modelo de trasnfer learning\n",
    "    base_model = red(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "    base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "    #Preprocessing input\n",
    "    train_ds = preprocess_input(train_ds) \n",
    "    test_ds = preprocess_input(test_ds)\n",
    "    \n",
    "    #Definir fine tunning\n",
    "    flatten_layer = layers.Flatten()\n",
    "    dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "    dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "    prediction_layer = layers.Dense(2, activation='softmax')\n",
    "\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        flatten_layer,\n",
    "        dense_layer_1,\n",
    "        dense_layer_2,\n",
    "        prediction_layer\n",
    "    ])\n",
    "    \n",
    "    #Definir compensador de pesos\n",
    "    classes = np.unique(train_labels)\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=classes, y=train_labels)\n",
    "    dic_class_weights = {0:class_weights[0], 1:class_weights[1]}\n",
    "    \n",
    "    #Entrenar el modelo\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20,  restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(train_ds, train_labels_categorical, epochs=200, validation_split=0.2, batch_size=32, callbacks=[es], class_weight=dic_class_weights)\n",
    "    \n",
    "    #Métricas de evaluación\n",
    "    score_test = model.evaluate(x=test_ds, y=test_labels_categorical, verbose = 0)\n",
    "    print(\"Test loss:\", score_test[0])\n",
    "    print(\"Test accuracy:\", score_test[1])\n",
    "    \n",
    "    predictions = model.predict(test_ds)\n",
    "    roc_score = roc_auc_score(test_labels_categorical, predictions, multi_class='ovr')\n",
    "    print(\"AUC score:\", roc_score)\n",
    "    \n",
    "    f1_score = f1_score(test_labels_categorical, predictions, average='weighted')\n",
    "    print(\"f1-score\", f1_score)\n",
    "    \n",
    "    matriz = confusion_matrix(test_labels_categorical, predictions)\n",
    "    print(\"Matriz de confusión\", matriz)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c44cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
